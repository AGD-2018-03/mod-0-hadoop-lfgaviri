{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuerde no agregar o quitar celdas en este notebook, ni modificar su tipo. Si lo hace, el sistema automaticamente lo calificará con cero punto cero (0.0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenga las letras asociadas a cada clave para el siguiente archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "0 \t C,F,A,B,D,I,H\n",
    "1 \t A,H,C,I,F,D,B\n",
    "2 \t B,H,I,F,G\n",
    "3 \t C,B,D\n",
    "4 \t D,C,I,G,H\n",
    "5 \t B,D,C,H,A\n",
    "6 \t H,D,C,B,G,F,D\n",
    "7 \t F,A,G,C,B,D,H,I\n",
    "8 \t F,A,I,B,D\n",
    "9 \t F,A,B,D,C,D,G,I\n",
    "10 \t D,B,A,C,H\n",
    "11 \t G,D,B,H,I,C,F\n",
    "12 \t D,D,C,F,B,A,H,G\n",
    "13 \t F,A,D\n",
    "14 \t D,A,C,G\n",
    "15 \t H,A,F,D,B,C,G,I\n",
    "16 \t A,I,B,D\n",
    "17 \t C,B,G,A,D,H,F\n",
    "18 \t I,B,A,H,D,F\n",
    "19 \t B,D,F,D,I\n",
    "20 \t C,B,H,F,I,G,D,D\n",
    "21 \t F,A,B,C,G,D\n",
    "22 \t I,G,F,C,H,B\n",
    "23 \t H,F,C,B,D,D,A\n",
    "24 \t F,D,G,A,H\n",
    "25 \t G,H,B,C,A,F,I\n",
    "26 \t G,F,B,A,H,D,D,I\n",
    "27 \t B,A,H,I,D,G,F\n",
    "28 \t A,H,D,F,C\n",
    "29 \t C,D,A,F,G,B,H,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#! /usr/bin/env python\n",
    "\n",
    "import sys\n",
    "if __name__ == \"__main__\":\n",
    "    for line in sys.stdin:\n",
    "        for num in line.split()[0:1]:\n",
    "            for grupo_letras in line.split()[1:2]:\n",
    "                for letra in grupo_letras.split(',')[0:]:\n",
    "                    sys.stdout.write(\"{}\\t{}\\n\".format(letra, num))\n",
    "##  >>> Write your answer here <<<\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "if __name__ == '__main__':\n",
    "    curkey = None\n",
    "    lista_valores = None\n",
    "    lines = sys.stdin.readlines()\n",
    "    ##lines.sort()\n",
    "    \n",
    "    for line in lines:\n",
    "        key, val = line.split(\"\\t\")\n",
    "        val = int(val)\n",
    "        if key == curkey: \n",
    "            lista_valores = str(lista_valores) + ',' + str(val)  \n",
    "        else:\n",
    "            if curkey is not None:\n",
    "                sys.stdout.write(\"{}\\t{}\\n\".format(curkey, lista_valores)) \n",
    "            curkey = key\n",
    "            lista_valores = val\n",
    "    sys.stdout.write(\"{}\\t{}\\n\".format(curkey, lista_valores))\n",
    "##  >>> Write your answer here <<<\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\t1,18,26,21,10,7,0,12,25,8,27,13,14,15,24,29,16,28,23,9,17,5\n",
      "B\t22,19,20,21,1,29,2,3,5,6,7,27,8,9,26,11,12,0,25,15,16,17,23,18,10\n",
      "C\t0,29,28,25,23,22,21,20,17,15,14,12,11,10,9,7,6,5,4,3,1\n",
      "D\t19,19,18,17,23,23,1,16,24,15,14,13,29,12,12,11,10,9,9,26,26,8,7,27,6,0,6,5,4,3,28,21,20,20,29\n",
      "F\t18,25,6,20,9,0,1,23,11,8,24,27,28,17,21,19,13,29,26,12,22,7,15,2\n",
      "G\t22,6,27,26,11,25,2,9,20,24,14,21,15,12,29,17,7,4\n",
      "H\t25,12,1,11,10,26,0,2,20,27,7,22,6,18,23,17,5,28,4,24,15,29\n",
      "I\t7,16,25,15,8,26,2,9,11,4,22,18,20,0,27,19,1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-27 16:23:01,433 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-10-27 16:23:02,138 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "2018-10-27 16:23:02,212 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2018-10-27 16:23:02,212 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2018-10-27 16:23:02,230 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-10-27 16:23:02,641 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2018-10-27 16:23:02,712 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2018-10-27 16:23:02,932 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local806534977_0001\n",
      "2018-10-27 16:23:02,934 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2018-10-27 16:23:03,112 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2018-10-27 16:23:03,115 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2018-10-27 16:23:03,115 INFO mapreduce.Job: Running job: job_local806534977_0001\n",
      "2018-10-27 16:23:03,117 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2018-10-27 16:23:03,122 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-10-27 16:23:03,122 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-10-27 16:23:03,186 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2018-10-27 16:23:03,190 INFO mapred.LocalJobRunner: Starting task: attempt_local806534977_0001_m_000000_0\n",
      "2018-10-27 16:23:03,216 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-10-27 16:23:03,216 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-10-27 16:23:03,226 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-10-27 16:23:03,226 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-10-27 16:23:03,237 INFO mapred.MapTask: Processing split: file:/Users/luisafernandagaviriaardila/UNAL/MAP_REDUCE/mod-0-hadoop-lfgaviri-master/input.txt:0+507\n",
      "2018-10-27 16:23:03,249 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2018-10-27 16:23:03,334 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2018-10-27 16:23:03,334 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2018-10-27 16:23:03,334 INFO mapred.MapTask: soft limit at 83886080\n",
      "2018-10-27 16:23:03,334 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2018-10-27 16:23:03,334 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2018-10-27 16:23:03,337 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2018-10-27 16:23:03,365 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/luisafernandagaviriaardila/UNAL/MAP_REDUCE/mod-0-hadoop-lfgaviri-master/./mapper.py]\n",
      "2018-10-27 16:23:03,372 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2018-10-27 16:23:03,372 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2018-10-27 16:23:03,373 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2018-10-27 16:23:03,373 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2018-10-27 16:23:03,374 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2018-10-27 16:23:03,374 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2018-10-27 16:23:03,374 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2018-10-27 16:23:03,375 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2018-10-27 16:23:03,375 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2018-10-27 16:23:03,376 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2018-10-27 16:23:03,376 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2018-10-27 16:23:03,376 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2018-10-27 16:23:03,400 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 16:23:03,400 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 16:23:03,454 INFO streaming.PipeMapRed: Records R/W=30/1\n",
      "2018-10-27 16:23:03,459 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-10-27 16:23:03,459 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-10-27 16:23:03,462 INFO mapred.LocalJobRunner: \n",
      "2018-10-27 16:23:03,462 INFO mapred.MapTask: Starting flush of map output\n",
      "2018-10-27 16:23:03,462 INFO mapred.MapTask: Spilling map output\n",
      "2018-10-27 16:23:03,462 INFO mapred.MapTask: bufstart = 0; bufend = 860; bufvoid = 104857600\n",
      "2018-10-27 16:23:03,462 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213664(104854656); length = 733/6553600\n",
      "2018-10-27 16:23:03,494 INFO mapred.MapTask: Finished spill 0\n",
      "2018-10-27 16:23:03,514 INFO mapred.Task: Task:attempt_local806534977_0001_m_000000_0 is done. And is in the process of committing\n",
      "2018-10-27 16:23:03,516 INFO mapred.LocalJobRunner: Records R/W=30/1\n",
      "2018-10-27 16:23:03,516 INFO mapred.Task: Task 'attempt_local806534977_0001_m_000000_0' done.\n",
      "2018-10-27 16:23:03,527 INFO mapred.Task: Final Counters for attempt_local806534977_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=177019\n",
      "\t\tFILE: Number of bytes written=701780\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=184\n",
      "\t\tMap output bytes=860\n",
      "\t\tMap output materialized bytes=1234\n",
      "\t\tInput split bytes=145\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=184\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=246939648\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=507\n",
      "2018-10-27 16:23:03,527 INFO mapred.LocalJobRunner: Finishing task: attempt_local806534977_0001_m_000000_0\n",
      "2018-10-27 16:23:03,528 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2018-10-27 16:23:03,530 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2018-10-27 16:23:03,530 INFO mapred.LocalJobRunner: Starting task: attempt_local806534977_0001_r_000000_0\n",
      "2018-10-27 16:23:03,544 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2018-10-27 16:23:03,544 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2018-10-27 16:23:03,544 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "2018-10-27 16:23:03,544 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "2018-10-27 16:23:03,547 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@76e6b99d\n",
      "2018-10-27 16:23:03,549 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2018-10-27 16:23:03,569 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=1336252800, maxSingleShuffleLimit=334063200, mergeThreshold=881926912, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2018-10-27 16:23:03,571 INFO reduce.EventFetcher: attempt_local806534977_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2018-10-27 16:23:03,594 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806534977_0001_m_000000_0 decomp: 1230 len: 1234 to MEMORY\n",
      "2018-10-27 16:23:03,601 INFO reduce.InMemoryMapOutput: Read 1230 bytes from map-output for attempt_local806534977_0001_m_000000_0\n",
      "2018-10-27 16:23:03,602 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1230, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1230\n",
      "2018-10-27 16:23:03,604 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2018-10-27 16:23:03,604 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-10-27 16:23:03,604 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2018-10-27 16:23:03,621 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-10-27 16:23:03,621 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1226 bytes\n",
      "2018-10-27 16:23:03,631 INFO reduce.MergeManagerImpl: Merged 1 segments, 1230 bytes to disk to satisfy reduce memory limit\n",
      "2018-10-27 16:23:03,632 INFO reduce.MergeManagerImpl: Merging 1 files, 1234 bytes from disk\n",
      "2018-10-27 16:23:03,633 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2018-10-27 16:23:03,633 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2018-10-27 16:23:03,634 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1226 bytes\n",
      "2018-10-27 16:23:03,634 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-10-27 16:23:03,642 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/luisafernandagaviriaardila/UNAL/MAP_REDUCE/mod-0-hadoop-lfgaviri-master/./reducer.py]\n",
      "2018-10-27 16:23:03,644 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2018-10-27 16:23:03,644 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2018-10-27 16:23:03,687 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 16:23:03,687 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 16:23:03,689 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2018-10-27 16:23:03,707 INFO streaming.PipeMapRed: Records R/W=184/1\n",
      "2018-10-27 16:23:03,711 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2018-10-27 16:23:03,711 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2018-10-27 16:23:03,712 INFO mapred.Task: Task:attempt_local806534977_0001_r_000000_0 is done. And is in the process of committing\n",
      "2018-10-27 16:23:03,713 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2018-10-27 16:23:03,713 INFO mapred.Task: Task attempt_local806534977_0001_r_000000_0 is allowed to commit now\n",
      "2018-10-27 16:23:03,715 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806534977_0001_r_000000_0' to file:/Users/luisafernandagaviriaardila/UNAL/MAP_REDUCE/mod-0-hadoop-lfgaviri-master/output\n",
      "2018-10-27 16:23:03,716 INFO mapred.LocalJobRunner: Records R/W=184/1 > reduce\n",
      "2018-10-27 16:23:03,716 INFO mapred.Task: Task 'attempt_local806534977_0001_r_000000_0' done.\n",
      "2018-10-27 16:23:03,716 INFO mapred.Task: Final Counters for attempt_local806534977_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=179519\n",
      "\t\tFILE: Number of bytes written=703534\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1234\n",
      "\t\tReduce input records=184\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=275251200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=520\n",
      "2018-10-27 16:23:03,717 INFO mapred.LocalJobRunner: Finishing task: attempt_local806534977_0001_r_000000_0\n",
      "2018-10-27 16:23:03,717 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2018-10-27 16:23:04,122 INFO mapreduce.Job: Job job_local806534977_0001 running in uber mode : false\n",
      "2018-10-27 16:23:04,123 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2018-10-27 16:23:04,125 INFO mapreduce.Job: Job job_local806534977_0001 completed successfully\n",
      "2018-10-27 16:23:04,136 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=356538\n",
      "\t\tFILE: Number of bytes written=1405314\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30\n",
      "\t\tMap output records=184\n",
      "\t\tMap output bytes=860\n",
      "\t\tMap output materialized bytes=1234\n",
      "\t\tInput split bytes=145\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=1234\n",
      "\t\tReduce input records=184\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=368\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=522190848\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=507\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=520\n",
      "2018-10-27 16:23:04,136 INFO streaming.StreamJob: Output directory: output\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf output\n",
    "STREAM=$HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar\n",
    "chmod +x mapper.py\n",
    "chmod +x reducer.py\n",
    "hadoop jar $STREAM -input input.txt -output output  -mapper mapper.py -reducer reducer.py\n",
    "cat output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mapper.py reducer.py output input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la evaluación automática de este libro:\n",
    "\n",
    "* Abra un Terminal.\n",
    "* Asegurece que esat en la misma carpeta que contiene este notebook.\n",
    "* Salve el notebook.\n",
    "* Ejecute el siguiente comando:\n",
    "\n",
    "      ./gradetool 11-Taller.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
